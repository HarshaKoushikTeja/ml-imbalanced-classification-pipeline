{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEE 520 ML Project - Binary Classification\n",
    "\n",
    "**Name:** Harsha Koushik Teja Aila  \n",
    "**Date:** January 2025\n",
    "\n",
    "## Goal\n",
    "Build a classifier to predict binary labels on imbalanced data. Need to minimize Balanced Error Rate (BER).\n",
    "\n",
    "## Dataset\n",
    "- 10,000 labeled samples for training\n",
    "- 10,000 unlabeled for predictions\n",
    "- 21 features (mix of ordinal, numerical, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "labeled = pd.read_excel(\"ProjectLABELED2025.xlsx\")\n",
    "unlabeled = pd.read_excel(\"ProjectNOTLABELED2025.xlsx\")\n",
    "\n",
    "print(f\"Labeled shape: {labeled.shape}\")\n",
    "print(f\"Unlabeled shape: {unlabeled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class balance\n",
    "print(labeled['label'].value_counts())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(labeled['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize class imbalance\n",
    "plt.figure(figsize=(8, 5))\n",
    "labeled['label'].value_counts().plot(kind='bar', color=['steelblue', 'coral'])\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(labeled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index columns and drop from working data\n",
    "labeled_index = labeled[\"Unnamed: 0\"]\n",
    "unlabeled_index = unlabeled[\"Unnamed: 0\"]\n",
    "\n",
    "labeled = labeled.drop(columns=[\"Unnamed: 0\"])\n",
    "unlabeled_clean = unlabeled.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Based on the project description:\n",
    "- x2, x3, x4 are ordinal\n",
    "- x15-x21 are numerical\n",
    "- Rest are binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature groups\n",
    "ordinal_cols = [\"x2\", \"x3\", \"x4\"]\n",
    "numeric_cols = [\"x15\",\"x16\",\"x17\",\"x18\",\"x19\",\"x20\",\"x21\"]\n",
    "binary_cols = [c for c in labeled.columns if c not in ordinal_cols + numeric_cols + [\"label\"]]\n",
    "\n",
    "print(f\"Ordinal: {ordinal_cols}\")\n",
    "print(f\"Numeric: {numeric_cols}\")\n",
    "print(f\"Binary: {binary_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at numerical features\n",
    "labeled[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix for numeric features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(labeled[numeric_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix - Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "Using ColumnTransformer to handle different feature types:\n",
    "- Ordinal: impute with mode, then ordinal encode\n",
    "- Numerical: impute with mean, then standardize\n",
    "- Binary: just impute with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"ord\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ]), ordinal_cols),\n",
    "\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), numeric_cols),\n",
    "\n",
    "    (\"bin\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "    ]), binary_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labeled.drop(columns=[\"label\"])\n",
    "y = labeled[\"label\"]\n",
    "\n",
    "# stratified split to maintain class balance\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Val size: {X_val.shape[0]}\")\n",
    "print(f\"\\nTrain class dist: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Val class dist: {y_val.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Trying 3 models:\n",
    "1. Random Forest\n",
    "2. SVM with RBF kernel\n",
    "3. Logistic Regression (baseline)\n",
    "\n",
    "Using GridSearchCV to tune hyperparameters. Scoring with balanced_accuracy since data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and param grids\n",
    "models = {\n",
    "    \"RandomForest\": (\n",
    "        Pipeline([(\"prep\", preprocessor), (\"clf\", RandomForestClassifier(random_state=42))]),\n",
    "        {\n",
    "            \"clf__n_estimators\": [200, 400],\n",
    "            \"clf__max_depth\": [None, 20],\n",
    "            \"clf__class_weight\": [None, \"balanced\"]\n",
    "        }\n",
    "    ),\n",
    "    \"SVM_RBF\": (\n",
    "        Pipeline([(\"prep\", preprocessor), (\"clf\", SVC())]),\n",
    "        {\n",
    "            \"clf__kernel\": [\"rbf\"],\n",
    "            \"clf__C\": [1.0],\n",
    "            \"clf__gamma\": [\"scale\"],\n",
    "            \"clf__class_weight\": [\"balanced\"]\n",
    "        }\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        Pipeline([(\"prep\", preprocessor), (\"clf\", LogisticRegression(max_iter=500))]),\n",
    "        {\n",
    "            \"clf__C\": [1.0],\n",
    "            \"clf__class_weight\": [\"balanced\"]\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate\n",
    "results = {}\n",
    "\n",
    "for name, (pipe, grid) in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipe, grid, scoring=\"balanced_accuracy\",\n",
    "        cv=cv, n_jobs=-1, verbose=2\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    ba = balanced_accuracy_score(y_val, y_pred)\n",
    "    ber = 1 - ba\n",
    "    \n",
    "    results[name] = {\n",
    "        \"best_cv\": grid_search.best_score_,\n",
    "        \"val_bal_acc\": ba,\n",
    "        \"val_BER\": ber,\n",
    "        \"confusion_matrix\": confusion_matrix(y_val, y_pred),\n",
    "        \"model\": best_model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Val Balanced Accuracy: {ba:.4f}\")\n",
    "    print(f\"Val BER: {ber:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_val, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV Bal Acc': [results[m]['best_cv'] for m in results.keys()],\n",
    "    'Val Bal Acc': [results[m]['val_bal_acc'] for m in results.keys()],\n",
    "    'Val BER': [results[m]['val_BER'] for m in results.keys()]\n",
    "}).sort_values('Val BER')\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, comparison['CV Bal Acc'], width, label='CV Bal Acc', alpha=0.8)\n",
    "ax.bar(x + width/2, comparison['Val Bal Acc'], width, label='Val Bal Acc', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Balanced Accuracy')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best model (lowest BER)\n",
    "best_name = min(results, key=lambda x: results[x][\"val_BER\"])\n",
    "final_model = results[best_name][\"model\"]\n",
    "\n",
    "print(f\"\\nBest model: {best_name}\")\n",
    "print(f\"Val BER: {results[best_name]['val_BER']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    cm = result['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f\"{name}\\nBER: {result['val_BER']:.4f}\")\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions\n",
    "\n",
    "Retrain best model on full dataset and generate predictions for unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain on full labeled data\n",
    "print(f\"Retraining {best_name} on full dataset...\")\n",
    "final_model.fit(X, y)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on unlabeled data\n",
    "final_predictions = final_model.predict(unlabeled_clean)\n",
    "\n",
    "print(f\"Generated {len(final_predictions)} predictions\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(pd.Series(final_predictions).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"index\": unlabeled_index,\n",
    "    \"label\": final_predictions\n",
    "})\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "output_filename = \"output/ProjectPredictions2025HarshaKoushikTejaAila.csv\"\n",
    "submission.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Tested 3 models with grid search\n",
    "- SVM with RBF kernel performed best\n",
    "- Used balanced accuracy to handle class imbalance\n",
    "- Final predictions saved for submission\n",
    "\n",
    "Possible improvements:\n",
    "- Try more hyperparameter values\n",
    "- Feature selection\n",
    "- Try ensemble methods (XGBoost, LightGBM)\n",
    "- SMOTE for handling imbalance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
